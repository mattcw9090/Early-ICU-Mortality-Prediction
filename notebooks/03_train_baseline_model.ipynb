{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7263d305",
   "metadata": {},
   "source": [
    "# 03 — Train Baseline Model (Logistic Regression)\n",
    "\n",
    "**Project:** Early ICU Mortality Prediction Using Structured EHR Data  \n",
    "**Dataset:** MIMIC-IV Clinical Database Demo (v2.2)\n",
    "\n",
    "## Goal of this notebook\n",
    "Train and evaluate a **baseline** mortality model using lab features from the first 24 hours of ICU admission.\n",
    "\n",
    "We will:\n",
    "1. Load `dataset_model_ready.csv` (cohort + label + lab features)\n",
    "2. Define feature columns and the label\n",
    "3. Split into train/test sets (patient-level split to reduce leakage across multiple stays)\n",
    "4. Train a baseline pipeline:\n",
    "   - Impute missing values\n",
    "   - Standardize features\n",
    "   - Logistic Regression (with class_weight='balanced')\n",
    "5. Evaluate:\n",
    "   - ROC-AUC\n",
    "   - PR-AUC (Average Precision)\n",
    "   - Confusion matrix at a chosen threshold\n",
    "   - Simple calibration check (optional plot)\n",
    "6. Save:\n",
    "   - `baseline_logreg.joblib`\n",
    "   - `baseline_metrics.json`\n",
    "\n",
    "## Inputs\n",
    "- `dataset_model_ready.csv`\n",
    "\n",
    "## Outputs\n",
    "- `baseline_logreg.joblib`\n",
    "- `baseline_metrics.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa00b53",
   "metadata": {},
   "source": [
    "## 1) Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\".\")\n",
    "\n",
    "if not (DATA_DIR / \"dataset_model_ready.csv\").exists():\n",
    "    alt = Path(\"/mnt/data\")\n",
    "    if (alt / \"dataset_model_ready.csv\").exists():\n",
    "        DATA_DIR = alt\n",
    "\n",
    "DATASET_PATH = DATA_DIR / \"dataset_model_ready.csv\"\n",
    "print(\"Dataset path:\", DATASET_PATH.resolve())\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print(\"Loaded dataset:\", df.shape)\n",
    "display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c63e7",
   "metadata": {},
   "source": [
    "## 2) Define label and feature columns\n",
    "\n",
    "- **Label:** `label_mortality`  \n",
    "- **Features:** lab aggregates and measured indicators (columns starting with `lab_`)\n",
    "\n",
    "We exclude identifiers and timestamps from the feature set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85fbbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL = \"label_mortality\"\n",
    "\n",
    "# Feature columns created in notebook 02\n",
    "feature_cols = [c for c in df.columns if c.startswith(\"lab_\")]\n",
    "\n",
    "# Basic checks\n",
    "assert LABEL_COL in df.columns, f\"Missing label column: {LABEL_COL}\"\n",
    "assert len(feature_cols) > 0, \"No lab feature columns found (expected columns starting with 'lab_')\"\n",
    "\n",
    "print(\"Num feature columns:\", len(feature_cols))\n",
    "print(\"Num rows:\", len(df))\n",
    "print(\"Label distribution:\")\n",
    "display(df[LABEL_COL].value_counts())\n",
    "\n",
    "# Drop rows with missing label (should not happen, but safe)\n",
    "df = df[df[LABEL_COL].notna()].copy()\n",
    "df[LABEL_COL] = df[LABEL_COL].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e163935",
   "metadata": {},
   "source": [
    "## 3) Train/test split (patient-level)\n",
    "\n",
    "To reduce information leakage when a patient has multiple ICU stays, we split by `subject_id`:\n",
    "- All stays of a patient go to either train or test, not both.\n",
    "\n",
    "> On the demo dataset, sizes are small, so results will be noisy. The goal is to validate the pipeline end-to-end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[LABEL_COL]\n",
    "groups = df[\"subject_id\"]\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "print(\"Train size:\", X_train.shape, \" Test size:\", X_test.shape)\n",
    "print(\"Train label distribution:\", y_train.value_counts().to_dict())\n",
    "print(\"Test label distribution:\", y_test.value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53154014",
   "metadata": {},
   "source": [
    "## 4) Baseline model pipeline\n",
    "\n",
    "We use a simple but strong baseline for tabular EHR data:\n",
    "- `SimpleImputer(median)` for missing values\n",
    "- `StandardScaler` for feature scaling\n",
    "- `LogisticRegression` with `class_weight='balanced'`\n",
    "\n",
    "This yields a solid starting point and is easy to interpret.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\",\n",
    "        solver=\"lbfgs\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2155c75",
   "metadata": {},
   "source": [
    "## 5) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0353b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "print(\"Trained baseline logistic regression ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd8bee2",
   "metadata": {},
   "source": [
    "## 6) Evaluate (ROC-AUC, PR-AUC, confusion matrix)\n",
    "\n",
    "We evaluate on the test set using predicted probabilities.\n",
    "\n",
    "- **ROC-AUC**: overall ranking quality\n",
    "- **PR-AUC (Average Precision)**: more informative for imbalanced outcomes\n",
    "- Confusion matrix at threshold 0.5 (then we also show the best F1 threshold as a reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc74305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, classification_report, precision_recall_curve\n",
    "\n",
    "# Predict probabilities for the positive class\n",
    "proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "roc = roc_auc_score(y_test, proba_test) if y_test.nunique() > 1 else np.nan\n",
    "pr  = average_precision_score(y_test, proba_test) if y_test.nunique() > 1 else np.nan\n",
    "\n",
    "print(f\"ROC-AUC: {roc:.3f}\")\n",
    "print(f\"PR-AUC (Avg Precision): {pr:.3f}\")\n",
    "\n",
    "# Threshold 0.5\n",
    "pred_05 = (proba_test >= 0.5).astype(int)\n",
    "cm_05 = confusion_matrix(y_test, pred_05, labels=[0, 1])\n",
    "print(\"\\nConfusion matrix @ 0.5 threshold (rows=true, cols=pred):\\n\", cm_05)\n",
    "print(\"\\nClassification report @ 0.5 threshold:\")\n",
    "print(classification_report(y_test, pred_05, digits=3))\n",
    "\n",
    "# Find threshold that maximizes F1 (for reference only)\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba_test)\n",
    "f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1)\n",
    "best_thr = thr[best_idx-1] if best_idx > 0 and best_idx-1 < len(thr) else 0.5\n",
    "\n",
    "print(f\"\\nBest-F1 threshold (reference): {best_thr:.3f}\")\n",
    "\n",
    "pred_best = (proba_test >= best_thr).astype(int)\n",
    "cm_best = confusion_matrix(y_test, pred_best, labels=[0, 1])\n",
    "print(\"Confusion matrix @ best-F1 threshold:\\n\", cm_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4071cf3d",
   "metadata": {},
   "source": [
    "## 7) Quick feature importance (coefficients)\n",
    "\n",
    "For logistic regression, the magnitude of coefficients (after scaling) gives a sense of which lab features push risk up/down.\n",
    "\n",
    "> These are *not* causal. They’re a quick sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract coefficients from the pipeline\n",
    "coef = model.named_steps[\"clf\"].coef_.ravel()\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"coef\": coef,\n",
    "    \"abs_coef\": np.abs(coef)\n",
    "}).sort_values(\"abs_coef\", ascending=False)\n",
    "\n",
    "display(coef_df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6d959",
   "metadata": {},
   "source": [
    "## 8) Save model + metrics\n",
    "\n",
    "We save:\n",
    "- the trained sklearn pipeline as `baseline_logreg.joblib`\n",
    "- metrics in `baseline_metrics.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818220d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "\n",
    "MODEL_PATH = Path(\"baseline_logreg.joblib\")\n",
    "METRICS_PATH = Path(\"baseline_metrics.json\")\n",
    "\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "\n",
    "metrics = {\n",
    "    \"n_rows\": int(len(df)),\n",
    "    \"n_features\": int(len(feature_cols)),\n",
    "    \"train_rows\": int(len(X_train)),\n",
    "    \"test_rows\": int(len(X_test)),\n",
    "    \"roc_auc\": None if np.isnan(roc) else float(roc),\n",
    "    \"pr_auc\": None if np.isnan(pr) else float(pr),\n",
    "    \"threshold_0.5_confusion_matrix\": cm_05.tolist(),\n",
    "    \"best_f1_threshold\": float(best_thr),\n",
    "    \"best_f1_confusion_matrix\": cm_best.tolist(),\n",
    "}\n",
    "\n",
    "with open(METRICS_PATH, \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" \", MODEL_PATH.resolve())\n",
    "print(\" \", METRICS_PATH.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
