{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e6c667",
   "metadata": {},
   "source": [
    "# 05 — Error Analysis & Case Review\n",
    "\n",
    "**Project:** Early ICU Mortality Prediction Using Structured EHR Data  \n",
    "**Dataset:** MIMIC-IV Clinical Database Demo (v2.2)\n",
    "\n",
    "## Goal of this notebook\n",
    "Go beyond aggregate metrics and answer:\n",
    "- *Which* cases does the model get wrong?\n",
    "- Are mistakes clustered (e.g., by low data availability, certain lab patterns)?\n",
    "- Are errors driven by missingness or extreme values?\n",
    "- What should we do next to improve the model?\n",
    "\n",
    "We will:\n",
    "1. Load test predictions (`eval_predictions.csv`) + threshold report (`threshold_report.csv`)\n",
    "2. Join predictions back to the model-ready dataset (`dataset_model_ready.csv`)\n",
    "3. Load `labevents.csv` to inspect raw time-stamped labs within the 0–24h window\n",
    "4. (Optional) Load `hosp/d_labitems.csv.gz` (if available) to map `itemid -> lab name`\n",
    "5. Create a small set of **case review tables** for:\n",
    "   - Top false positives (high risk predicted, survived)\n",
    "   - Top false negatives (low risk predicted, died)\n",
    "6. Summarize common error patterns and produce an “action list” for improvements\n",
    "\n",
    "## Inputs\n",
    "- `eval_predictions.csv`\n",
    "- `threshold_report.csv`\n",
    "- `dataset_model_ready.csv`\n",
    "- `labevents.csv`\n",
    "- (optional) `d_labitems.csv` or `d_labitems.csv.gz` (to label labs)\n",
    "\n",
    "## Outputs\n",
    "- `case_review_false_positives.csv`\n",
    "- `case_review_false_negatives.csv`\n",
    "- `error_analysis_summary.md` (a short write-up you can paste into README)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8fb33f",
   "metadata": {},
   "source": [
    "## 1) Resolve paths and load artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a2c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_dir():\n",
    "    d = Path(\".\")\n",
    "    if (d / \"eval_predictions.csv\").exists():\n",
    "        return d\n",
    "    alt = Path(\"/mnt/data\")\n",
    "    if (alt / \"eval_predictions.csv\").exists():\n",
    "        return alt\n",
    "    return d\n",
    "\n",
    "DATA_DIR = resolve_dir()\n",
    "print(\"Using DATA_DIR:\", DATA_DIR.resolve())\n",
    "\n",
    "PRED_PATH = DATA_DIR / \"eval_predictions.csv\"\n",
    "THR_PATH = DATA_DIR / \"threshold_report.csv\"\n",
    "DATASET_PATH = DATA_DIR / \"dataset_model_ready.csv\"\n",
    "LABEVENTS_PATH = DATA_DIR / \"labevents.csv\"\n",
    "\n",
    "for p in [PRED_PATH, THR_PATH, DATASET_PATH, LABEVENTS_PATH]:\n",
    "    assert p.exists(), f\"Missing required file: {p}\"\n",
    "\n",
    "pred = pd.read_csv(PRED_PATH)\n",
    "thr = pd.read_csv(THR_PATH)\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "labevents = pd.read_csv(LABEVENTS_PATH)\n",
    "\n",
    "print(\"Loaded:\")\n",
    "print(\"  pred:\", pred.shape)\n",
    "print(\"  thr:\", thr.shape)\n",
    "print(\"  dataset:\", df.shape)\n",
    "print(\"  labevents:\", labevents.shape)\n",
    "\n",
    "display(pred.head(5))\n",
    "display(thr.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4856c0",
   "metadata": {},
   "source": [
    "## 2) Join predictions back to the dataset\n",
    "\n",
    "This gives us the full feature vector and metadata for each evaluated ICU stay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL = \"label_mortality\"\n",
    "assert LABEL_COL in df.columns, \"dataset_model_ready.csv must contain label_mortality\"\n",
    "\n",
    "df_merged = pred.merge(\n",
    "    df,\n",
    "    on=[\"subject_id\", \"hadm_id\", \"stay_id\", LABEL_COL],\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_one\"\n",
    ")\n",
    "\n",
    "print(\"Merged rows:\", df_merged.shape)\n",
    "missing_rows = df_merged.isna().all(axis=1).sum()\n",
    "print(\"All-null rows after merge (should be 0):\", missing_rows)\n",
    "\n",
    "display(df_merged.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a63d52",
   "metadata": {},
   "source": [
    "## 3) Select cases for review (false positives / false negatives)\n",
    "\n",
    "We'll use the **same threshold definition** as in notebook 04:\n",
    "- default `threshold = 0.5`\n",
    "- but we also show the best-F1 threshold from the threshold report for context.\n",
    "\n",
    "Then we select:\n",
    "- Top `K` false positives (highest predicted risk among survivors)\n",
    "- Top `K` false negatives (lowest predicted risk among deaths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a default threshold\n",
    "DEFAULT_THRESHOLD = 0.5\n",
    "\n",
    "# Best-F1 threshold from report (if present)\n",
    "best_row = thr.loc[thr[\"f1\"].idxmax()]\n",
    "best_thr = float(best_row[\"threshold\"])\n",
    "print(\"Best-F1 threshold (from threshold_report.csv):\", best_thr)\n",
    "\n",
    "# Label predictions at default threshold\n",
    "df_merged[\"pred_label\"] = (df_merged[\"pred_proba\"] >= DEFAULT_THRESHOLD).astype(int)\n",
    "\n",
    "fp = df_merged[(df_merged[LABEL_COL] == 0) & (df_merged[\"pred_label\"] == 1)].copy()\n",
    "fn = df_merged[(df_merged[LABEL_COL] == 1) & (df_merged[\"pred_label\"] == 0)].copy()\n",
    "\n",
    "print(\"False positives:\", len(fp))\n",
    "print(\"False negatives:\", len(fn))\n",
    "\n",
    "K = 10\n",
    "fp_top = fp.sort_values(\"pred_proba\", ascending=False).head(K)\n",
    "fn_top = fn.sort_values(\"pred_proba\", ascending=True).head(K)\n",
    "\n",
    "display(fp_top[[\"subject_id\", \"hadm_id\", \"stay_id\", LABEL_COL, \"pred_proba\"]])\n",
    "display(fn_top[[\"subject_id\", \"hadm_id\", \"stay_id\", LABEL_COL, \"pred_proba\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf8205b",
   "metadata": {},
   "source": [
    "## 4) Optional: map lab `itemid` to names via `d_labitems`\n",
    "\n",
    "If you have `d_labitems.csv` (or `d_labitems.csv.gz`) in your folder, we'll load it.\n",
    "Otherwise, we’ll proceed using itemids only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbaa564",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_labitems = None\n",
    "\n",
    "candidates = [\n",
    "    DATA_DIR / \"d_labitems.csv\",\n",
    "    DATA_DIR / \"d_labitems.csv.gz\",\n",
    "    # if user still has the original folder structure:\n",
    "    DATA_DIR / \"hosp\" / \"d_labitems.csv.gz\",\n",
    "    DATA_DIR / \"hosp\" / \"d_labitems.csv\",\n",
    "]\n",
    "\n",
    "for c in candidates:\n",
    "    if c.exists():\n",
    "        if c.suffix == \".gz\":\n",
    "            d_labitems = pd.read_csv(c, compression=\"gzip\")\n",
    "        else:\n",
    "            d_labitems = pd.read_csv(c)\n",
    "        print(\"Loaded d_labitems from:\", c.resolve())\n",
    "        break\n",
    "\n",
    "if d_labitems is None:\n",
    "    print(\"d_labitems not found; continuing with itemid only.\")\n",
    "else:\n",
    "    # Standard columns: itemid, label, fluid, category, etc.\n",
    "    display(d_labitems.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6875d84d",
   "metadata": {},
   "source": [
    "## 5) Build a simple \"chart pack\" for each case\n",
    "\n",
    "For each selected ICU stay:\n",
    "- Pull **raw** lab events within the leakage-safe window:\n",
    "  `intime <= charttime <= prediction_time`\n",
    "- Summarize:\n",
    "  - top labs by absolute abnormality (z-score within that case) — simple heuristic\n",
    "  - count of labs measured\n",
    "  - earliest and latest labs\n",
    "\n",
    "This is a lightweight case-review aid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse times needed for window filtering\n",
    "for c in [\"intime\", \"prediction_time\"]:\n",
    "    if c in df_merged.columns:\n",
    "        df_merged[c] = pd.to_datetime(df_merged[c], errors=\"coerce\")\n",
    "\n",
    "labevents[\"charttime\"] = pd.to_datetime(labevents[\"charttime\"], errors=\"coerce\")\n",
    "labevents[\"valuenum\"] = pd.to_numeric(labevents[\"valuenum\"], errors=\"coerce\")\n",
    "\n",
    "# Helper: lab window for a single stay\n",
    "def labs_for_stay(stay_row):\n",
    "    subject_id = int(stay_row[\"subject_id\"])\n",
    "    hadm_id = int(stay_row[\"hadm_id\"])\n",
    "    intime = stay_row.get(\"intime\", pd.NaT)\n",
    "    ptime = stay_row.get(\"prediction_time\", pd.NaT)\n",
    "    if pd.isna(intime) or pd.isna(ptime):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    labs = labevents[(labevents[\"subject_id\"] == subject_id) & (labevents[\"hadm_id\"] == hadm_id)].copy()\n",
    "    labs = labs[labs[\"charttime\"].notna()]\n",
    "    labs = labs[(labs[\"charttime\"] >= intime) & (labs[\"charttime\"] <= ptime)]\n",
    "    labs = labs[labs[\"valuenum\"].notna()].copy()\n",
    "\n",
    "    if d_labitems is not None and \"itemid\" in labs.columns and \"itemid\" in d_labitems.columns:\n",
    "        labs = labs.merge(d_labitems[[\"itemid\", \"label\"]], on=\"itemid\", how=\"left\")\n",
    "    return labs\n",
    "\n",
    "# Helper: within-case lab summary\n",
    "def summarize_labs(labs):\n",
    "    if labs.empty:\n",
    "        return pd.DataFrame(), {\"n_events\": 0, \"n_unique_labs\": 0}\n",
    "    # Heuristic: within-case zscore per itemid (if multiple values)\n",
    "    labs2 = labs.copy()\n",
    "    labs2[\"lab_name\"] = labs2[\"label\"] if \"label\" in labs2.columns else labs2[\"itemid\"].astype(str)\n",
    "    grp = labs2.groupby(\"lab_name\")[\"valuenum\"]\n",
    "    labs2[\"z\"] = (labs2[\"valuenum\"] - grp.transform(\"mean\")) / (grp.transform(\"std\") + 1e-12)\n",
    "    labs2[\"abs_z\"] = labs2[\"z\"].abs()\n",
    "\n",
    "    # Summary per lab_name\n",
    "    summ = (labs2.groupby(\"lab_name\")[\"valuenum\"]\n",
    "            .agg([\"count\", \"min\", \"max\", \"mean\"])\n",
    "            .reset_index())\n",
    "    # Add a \"max abs z\" score as a crude salience marker\n",
    "    zmax = labs2.groupby(\"lab_name\")[\"abs_z\"].max().reset_index().rename(columns={\"abs_z\": \"max_abs_z\"})\n",
    "    summ = summ.merge(zmax, on=\"lab_name\", how=\"left\").sort_values([\"max_abs_z\", \"count\"], ascending=False)\n",
    "\n",
    "    stats = {\"n_events\": int(len(labs2)), \"n_unique_labs\": int(labs2[\"lab_name\"].nunique())}\n",
    "    return summ, stats\n",
    "\n",
    "# Example: build a pack for the first FP if any\n",
    "if len(fp_top) > 0:\n",
    "    row0 = fp_top.iloc[0]\n",
    "    labs0 = labs_for_stay(row0)\n",
    "    summ0, stats0 = summarize_labs(labs0)\n",
    "    print(\"Example case:\", int(row0[\"stay_id\"]), \"pred_proba:\", float(row0[\"pred_proba\"]), \"label:\", int(row0[LABEL_COL]))\n",
    "    print(\"Lab stats:\", stats0)\n",
    "    display(summ0.head(15))\n",
    "    display(labs0.sort_values(\"charttime\").head(20))\n",
    "else:\n",
    "    print(\"No false positives found at the default threshold.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adce30c",
   "metadata": {},
   "source": [
    "## 6) Generate case review tables for top false positives and false negatives\n",
    "\n",
    "For each case, we store:\n",
    "- identifiers + true label + predicted probability\n",
    "- number of labs observed in the window\n",
    "- number of unique lab tests\n",
    "- top 10 lab summaries (as a JSON string for portability)\n",
    "\n",
    "This makes it easy to review cases and share results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def build_case_review(df_cases, tag, top_labs=10):\n",
    "    rows = []\n",
    "    for _, r in df_cases.iterrows():\n",
    "        labs = labs_for_stay(r)\n",
    "        summ, stats = summarize_labs(labs)\n",
    "        top = summ.head(top_labs).to_dict(orient=\"records\") if not summ.empty else []\n",
    "        rows.append({\n",
    "            \"tag\": tag,\n",
    "            \"subject_id\": int(r[\"subject_id\"]),\n",
    "            \"hadm_id\": int(r[\"hadm_id\"]),\n",
    "            \"stay_id\": int(r[\"stay_id\"]),\n",
    "            \"label_mortality\": int(r[LABEL_COL]),\n",
    "            \"pred_proba\": float(r[\"pred_proba\"]),\n",
    "            \"n_lab_events_0_24h\": int(stats[\"n_events\"]),\n",
    "            \"n_unique_labs_0_24h\": int(stats[\"n_unique_labs\"]),\n",
    "            \"top_labs_summary\": json.dumps(top),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "fp_review = build_case_review(fp_top, \"false_positive\")\n",
    "fn_review = build_case_review(fn_top, \"false_negative\")\n",
    "\n",
    "display(fp_review.head())\n",
    "display(fn_review.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e300838b",
   "metadata": {},
   "source": [
    "## 7) Look for common error patterns (lightweight)\n",
    "\n",
    "We compare false positives vs false negatives on:\n",
    "- predicted probability distribution\n",
    "- lab measurement density (how much data is available)\n",
    "- missingness (how many 'measured' indicators are 0)\n",
    "\n",
    "This is meant to produce *actionable next steps*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurement density comparison\n",
    "combined = pd.concat([fp_review, fn_review], ignore_index=True)\n",
    "\n",
    "if not combined.empty:\n",
    "    display(combined.groupby(\"tag\")[[\"pred_proba\", \"n_lab_events_0_24h\", \"n_unique_labs_0_24h\"]].describe())\n",
    "\n",
    "# Missingness summary from model-ready dataset (measured indicators)\n",
    "measured_cols = [c for c in df_merged.columns if c.endswith(\"_measured\") and c.startswith(\"lab_\")]\n",
    "if measured_cols:\n",
    "    # Only for the cases we are reviewing\n",
    "    case_ids = set(combined[\"stay_id\"].tolist()) if not combined.empty else set()\n",
    "    cases_full = df_merged[df_merged[\"stay_id\"].isin(case_ids)].copy()\n",
    "    if not cases_full.empty:\n",
    "        cases_full[\"tag\"] = np.where(\n",
    "            (cases_full[LABEL_COL] == 0) & (cases_full[\"pred_label\"] == 1),\n",
    "            \"false_positive\",\n",
    "            np.where((cases_full[LABEL_COL] == 1) & (cases_full[\"pred_label\"] == 0), \"false_negative\", \"other\")\n",
    "        )\n",
    "        miss_rate = (cases_full[measured_cols] == 0).mean(axis=1)\n",
    "        cases_full[\"missingness_rate_measured\"] = miss_rate\n",
    "        display(cases_full.groupby(\"tag\")[\"missingness_rate_measured\"].describe())\n",
    "else:\n",
    "    print(\"No measured indicator columns found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c212c",
   "metadata": {},
   "source": [
    "## 8) Write a short error analysis summary (Markdown)\n",
    "\n",
    "This generates a concise summary you can paste into your README.\n",
    "\n",
    "It includes:\n",
    "- How many FPs/FNs at threshold 0.5\n",
    "- A quick note on measurement density\n",
    "- Suggested next improvements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb292d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_lines = []\n",
    "summary_lines.append(\"# Error Analysis Summary (Baseline Logistic Regression)\")\n",
    "summary_lines.append(\"\")\n",
    "summary_lines.append(f\"- Threshold used for review: **{DEFAULT_THRESHOLD:.2f}**\")\n",
    "summary_lines.append(f\"- False positives: **{len(fp)}** (survived, predicted high risk)\")\n",
    "summary_lines.append(f\"- False negatives: **{len(fn)}** (died, predicted low risk)\")\n",
    "summary_lines.append(\"\")\n",
    "\n",
    "if not combined.empty:\n",
    "    for tag in [\"false_positive\", \"false_negative\"]:\n",
    "        sub = combined[combined[\"tag\"] == tag]\n",
    "        if not sub.empty:\n",
    "            summary_lines.append(f\"## {tag.replace('_',' ').title()}\")\n",
    "            summary_lines.append(f\"- Mean predicted risk: **{sub['pred_proba'].mean():.3f}**\")\n",
    "            summary_lines.append(f\"- Mean lab events in 0–24h: **{sub['n_lab_events_0_24h'].mean():.1f}**\")\n",
    "            summary_lines.append(f\"- Mean unique labs in 0–24h: **{sub['n_unique_labs_0_24h'].mean():.1f}**\")\n",
    "            summary_lines.append(\"\")\n",
    "\n",
    "summary_lines.append(\"## Hypotheses for errors\")\n",
    "summary_lines.append(\"- Some errors may be driven by **limited lab coverage** (missingness) in the first 24 hours.\")\n",
    "summary_lines.append(\"- Some false positives may represent **high acuity survivors** (treated effectively) — mortality is not the only notion of risk.\")\n",
    "summary_lines.append(\"- Some false negatives may require **vitals/clinical context** not present in labs alone.\")\n",
    "summary_lines.append(\"\")\n",
    "summary_lines.append(\"## Next improvements (action list)\")\n",
    "summary_lines.append(\"1. Add **vitals** from `icu.chartevents` (first 24h) and compare.\")\n",
    "summary_lines.append(\"2. Add `d_labitems` labels and build a **human-readable** report of top contributing labs.\")\n",
    "summary_lines.append(\"3. Try a stronger model for tabular data (e.g., **HistGradientBoosting** or **XGBoost/LightGBM**).\")\n",
    "summary_lines.append(\"4. Use time-aware splits when moving to full MIMIC-IV.\")\n",
    "summary_lines.append(\"\")\n",
    "\n",
    "summary_md = \"\\n\".join(summary_lines)\n",
    "print(summary_md)\n",
    "\n",
    "OUT_MD = Path(\"error_analysis_summary.md\")\n",
    "OUT_MD.write_text(summary_md, encoding=\"utf-8\")\n",
    "print(\"\\nSaved:\", OUT_MD.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6210a",
   "metadata": {},
   "source": [
    "## 9) Save case review CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP_OUT = Path(\"case_review_false_positives.csv\")\n",
    "FN_OUT = Path(\"case_review_false_negatives.csv\")\n",
    "\n",
    "fp_review.to_csv(FP_OUT, index=False)\n",
    "fn_review.to_csv(FN_OUT, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" \", FP_OUT.resolve())\n",
    "print(\" \", FN_OUT.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
