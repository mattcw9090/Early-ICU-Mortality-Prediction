{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32864090",
   "metadata": {},
   "source": [
    "# 04 — Model Evaluation & Error Analysis\n",
    "\n",
    "**Project:** Early ICU Mortality Prediction Using Structured EHR Data  \n",
    "**Dataset:** MIMIC-IV Clinical Database Demo (v2.2)\n",
    "\n",
    "## Goal of this notebook\n",
    "Evaluate the baseline mortality model from `03_train_baseline_model.ipynb` and understand its behavior.\n",
    "\n",
    "We will:\n",
    "1. Load the saved model and metrics\n",
    "2. Recompute predictions on a **patient-level** held-out split (to reduce leakage)\n",
    "3. Plot:\n",
    "   - ROC curve\n",
    "   - Precision–Recall curve\n",
    "   - Calibration curve\n",
    "   - Predicted probability distributions by class\n",
    "4. Do threshold analysis\n",
    "5. Perform simple error analysis (top false positives / false negatives + feature contribution breakdown)\n",
    "6. (Optional) Subgroup checks if `patients.csv` is available (e.g., by sex, age bins)\n",
    "\n",
    "## Inputs\n",
    "- `baseline_logreg.joblib`\n",
    "- `baseline_metrics.json` (optional, for comparison)\n",
    "- `dataset_model_ready.csv`\n",
    "\n",
    "## Outputs\n",
    "- `eval_predictions.csv` (test split predictions)\n",
    "- `threshold_report.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c717cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7c3cc",
   "metadata": {},
   "source": [
    "## 1) Load artifacts (dataset, model, optional metrics)\n",
    "\n",
    "We use a small helper to resolve paths whether you run this notebook:\n",
    "- in the same folder as the CSVs, or\n",
    "- in an environment where files are in `/mnt/data`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_dir():\n",
    "    d = Path(\".\")\n",
    "    if (d / \"dataset_model_ready.csv\").exists():\n",
    "        return d\n",
    "    alt = Path(\"/mnt/data\")\n",
    "    if (alt / \"dataset_model_ready.csv\").exists():\n",
    "        return alt\n",
    "    return d\n",
    "\n",
    "DATA_DIR = resolve_dir()\n",
    "print(\"Using DATA_DIR:\", DATA_DIR.resolve())\n",
    "\n",
    "DATASET_PATH = DATA_DIR / \"dataset_model_ready.csv\"\n",
    "MODEL_PATH = DATA_DIR / \"baseline_logreg.joblib\"\n",
    "METRICS_PATH = DATA_DIR / \"baseline_metrics.json\"\n",
    "\n",
    "assert DATASET_PATH.exists(), f\"Missing {DATASET_PATH}\"\n",
    "assert MODEL_PATH.exists(), f\"Missing {MODEL_PATH}\"\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "import joblib, json\n",
    "model = joblib.load(MODEL_PATH)\n",
    "\n",
    "saved_metrics = None\n",
    "if METRICS_PATH.exists():\n",
    "    with open(METRICS_PATH, \"r\") as f:\n",
    "        saved_metrics = json.load(f)\n",
    "\n",
    "print(\"Loaded dataset:\", df.shape)\n",
    "print(\"Loaded model:\", type(model))\n",
    "if saved_metrics:\n",
    "    print(\"Found baseline_metrics.json ✅\")\n",
    "    display(pd.DataFrame([saved_metrics]))\n",
    "else:\n",
    "    print(\"baseline_metrics.json not found (that's okay).\")\n",
    "\n",
    "display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c7bb2",
   "metadata": {},
   "source": [
    "## 2) Define label and feature columns\n",
    "\n",
    "- **Label:** `label_mortality`\n",
    "- **Features:** columns starting with `lab_`\n",
    "\n",
    "We exclude identifiers and timestamps from modeling features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a210fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL = \"label_mortality\"\n",
    "feature_cols = [c for c in df.columns if c.startswith(\"lab_\")]\n",
    "\n",
    "assert LABEL_COL in df.columns, f\"Missing label column: {LABEL_COL}\"\n",
    "assert len(feature_cols) > 0, \"No feature columns found (expected columns starting with 'lab_')\"\n",
    "\n",
    "# Clean label\n",
    "df = df[df[LABEL_COL].notna()].copy()\n",
    "df[LABEL_COL] = df[LABEL_COL].astype(int)\n",
    "\n",
    "# For determinism across runs\n",
    "sort_cols = [c for c in [\"subject_id\", \"hadm_id\", \"stay_id\"] if c in df.columns]\n",
    "df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[LABEL_COL]\n",
    "groups = df[\"subject_id\"]\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Features:\", len(feature_cols))\n",
    "print(\"Label distribution:\")\n",
    "display(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc731824",
   "metadata": {},
   "source": [
    "## 3) Recreate a patient-level train/test split\n",
    "\n",
    "We use the same strategy as the baseline notebook:\n",
    "- **GroupShuffleSplit** by `subject_id` (patient)\n",
    "\n",
    "This avoids putting multiple ICU stays from the same patient into both train and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c73b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_test = X.iloc[test_idx]\n",
    "y_test = y.iloc[test_idx]\n",
    "df_test = df.iloc[test_idx].copy()\n",
    "\n",
    "print(\"Test rows:\", len(df_test))\n",
    "print(\"Test label distribution:\", y_test.value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42db231",
   "metadata": {},
   "source": [
    "## 4) Predict and compute headline metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d93011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "roc = roc_auc_score(y_test, proba_test) if y_test.nunique() > 1 else np.nan\n",
    "pr  = average_precision_score(y_test, proba_test) if y_test.nunique() > 1 else np.nan\n",
    "\n",
    "print(f\"ROC-AUC: {roc:.3f}\")\n",
    "print(f\"PR-AUC (Average Precision): {pr:.3f}\")\n",
    "\n",
    "df_test[\"pred_proba\"] = proba_test\n",
    "display(df_test[[\"subject_id\", \"hadm_id\", \"stay_id\", LABEL_COL, \"pred_proba\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029ad65",
   "metadata": {},
   "source": [
    "## 5) ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf48a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, proba_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Test Split)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b58513e",
   "metadata": {},
   "source": [
    "## 6) Precision–Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, proba_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve (Test Split)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b977d",
   "metadata": {},
   "source": [
    "## 7) Calibration curve (reliability)\n",
    "\n",
    "Calibration answers: *when the model says 0.7 risk, does ~70% actually happen?*\n",
    "\n",
    "With small demo data, calibration will be noisy — treat this as a diagnostic, not a final statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_test, proba_test, n_bins=5, strategy=\"quantile\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.xlabel(\"Mean Predicted Probability\")\n",
    "plt.ylabel(\"Fraction of Positives\")\n",
    "plt.title(\"Calibration Curve (Test Split)\")\n",
    "plt.show()\n",
    "\n",
    "cal_df = pd.DataFrame({\"bin_mean_pred\": prob_pred, \"bin_frac_pos\": prob_true})\n",
    "display(cal_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d579935",
   "metadata": {},
   "source": [
    "## 8) Predicted probability distributions by class\n",
    "\n",
    "This helps visualize separation (or overlap) between survivors and non-survivors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd601768",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(df_test.loc[df_test[LABEL_COL] == 0, \"pred_proba\"].dropna(), bins=10, alpha=0.8, label=\"label=0\")\n",
    "plt.hist(df_test.loc[df_test[LABEL_COL] == 1, \"pred_proba\"].dropna(), bins=10, alpha=0.8, label=\"label=1\")\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Predicted Probabilities by Class (Test Split)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f206f093",
   "metadata": {},
   "source": [
    "## 9) Threshold analysis\n",
    "\n",
    "We compute precision/recall/F1 across thresholds, and highlight:\n",
    "- `threshold = 0.5` (default)\n",
    "- best-F1 threshold (on the test split, for diagnostics)\n",
    "\n",
    "> In real settings, threshold choice should reflect the cost of false positives vs false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def metrics_at_threshold(y_true, proba, thr):\n",
    "    pred = (proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, pred, labels=[0, 1]).ravel()\n",
    "    precision = tp / (tp + fp + 1e-12)\n",
    "    recall = tp / (tp + fn + 1e-12)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "    return {\n",
    "        \"threshold\": thr,\n",
    "        \"tp\": int(tp), \"fp\": int(fp), \"tn\": int(tn), \"fn\": int(fn),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "    }\n",
    "\n",
    "thresholds = np.linspace(0.05, 0.95, 19)\n",
    "report = pd.DataFrame([metrics_at_threshold(y_test.values, proba_test, float(t)) for t in thresholds])\n",
    "\n",
    "best_row = report.loc[report[\"f1\"].idxmax()]\n",
    "print(\"Best-F1 threshold on test split:\", float(best_row[\"threshold\"]))\n",
    "\n",
    "display(report.sort_values(\"f1\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0ae77",
   "metadata": {},
   "source": [
    "## 10) Error analysis: top false positives / false negatives\n",
    "\n",
    "We list the most confident mistakes on the test split:\n",
    "- False positives: high predicted risk but survived\n",
    "- False negatives: low predicted risk but died\n",
    "\n",
    "Then we provide a small helper to explain *which features* contribute most to the prediction for a specific stay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe21b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label predictions at a chosen threshold (use 0.5 for consistency)\n",
    "THRESHOLD = 0.5\n",
    "df_test[\"pred_label\"] = (df_test[\"pred_proba\"] >= THRESHOLD).astype(int)\n",
    "\n",
    "fp = df_test[(df_test[LABEL_COL] == 0) & (df_test[\"pred_label\"] == 1)].sort_values(\"pred_proba\", ascending=False)\n",
    "fn = df_test[(df_test[LABEL_COL] == 1) & (df_test[\"pred_label\"] == 0)].sort_values(\"pred_proba\", ascending=True)\n",
    "\n",
    "print(\"False positives:\", len(fp))\n",
    "print(\"False negatives:\", len(fn))\n",
    "\n",
    "display(fp[[\"subject_id\", \"hadm_id\", \"stay_id\", LABEL_COL, \"pred_proba\"]].head(10))\n",
    "display(fn[[\"subject_id\", \"hadm_id\", \"stay_id\", LABEL_COL, \"pred_proba\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae4ec56",
   "metadata": {},
   "source": [
    "### Explain a prediction (feature contributions)\n",
    "\n",
    "For logistic regression, we can decompose the log-odds into per-feature contributions:\n",
    "- Transform features through the model’s preprocessing (imputer + scaler)\n",
    "- Multiply by the classifier coefficients\n",
    "\n",
    "This is a **debugging aid**, not a clinical explanation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f847922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_stay(stay_id, top_k=10):\n",
    "    # Extract single row\n",
    "    row = df_test[df_test[\"stay_id\"] == stay_id]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"stay_id {stay_id} not found in test split\")\n",
    "    x = row[feature_cols]\n",
    "\n",
    "    # Transform through preprocessing\n",
    "    x_imp = model.named_steps[\"imputer\"].transform(x)\n",
    "    x_scaled = model.named_steps[\"scaler\"].transform(x_imp)\n",
    "\n",
    "    coef = model.named_steps[\"clf\"].coef_.ravel()\n",
    "    intercept = float(model.named_steps[\"clf\"].intercept_.ravel()[0])\n",
    "\n",
    "    contrib = x_scaled.ravel() * coef\n",
    "    contrib_df = pd.DataFrame({\n",
    "        \"feature\": feature_cols,\n",
    "        \"value_raw\": x.iloc[0].values,\n",
    "        \"contribution_to_logodds\": contrib,\n",
    "        \"abs_contribution\": np.abs(contrib)\n",
    "    }).sort_values(\"abs_contribution\", ascending=False)\n",
    "\n",
    "    # Reconstruct logit/prob (sanity check)\n",
    "    logit = intercept + contrib.sum()\n",
    "    prob = 1 / (1 + np.exp(-logit))\n",
    "\n",
    "    meta = row[[\"subject_id\", \"hadm_id\", \"stay_id\", LABEL_COL, \"pred_proba\"]].copy()\n",
    "    meta[\"reconstructed_prob\"] = prob\n",
    "\n",
    "    return meta, contrib_df.head(top_k)\n",
    "\n",
    "# Example: explain the most confident false positive (if any)\n",
    "if len(fp) > 0:\n",
    "    example_stay = int(fp.iloc[0][\"stay_id\"])\n",
    "    meta, top_contrib = explain_stay(example_stay, top_k=12)\n",
    "    display(meta)\n",
    "    display(top_contrib)\n",
    "else:\n",
    "    print(\"No false positives at threshold=0.5 to explain.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af045f1",
   "metadata": {},
   "source": [
    "## 11) Optional subgroup checks (if `patients.csv` is available)\n",
    "\n",
    "If you have `patients.csv` in the same folder, we can join it to get:\n",
    "- `gender`\n",
    "- `anchor_age` (de-identified age)\n",
    "\n",
    "Then compute metrics by subgroup.\n",
    "\n",
    "This section is optional and will skip automatically if the file is not present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENTS_PATH = DATA_DIR / \"patients.csv\"\n",
    "if PATIENTS_PATH.exists():\n",
    "    patients = pd.read_csv(PATIENTS_PATH)[[\"subject_id\", \"gender\", \"anchor_age\"]]\n",
    "    df_test_sub = df_test.merge(patients, on=\"subject_id\", how=\"left\")\n",
    "\n",
    "    # Age bins (rough; demo is small)\n",
    "    df_test_sub[\"age_bin\"] = pd.cut(df_test_sub[\"anchor_age\"], bins=[0, 40, 60, 80, 120], right=False)\n",
    "\n",
    "    def group_metrics(frame):\n",
    "        yt = frame[LABEL_COL].values\n",
    "        pt = frame[\"pred_proba\"].values\n",
    "        out = {\n",
    "            \"n\": len(frame),\n",
    "            \"positives\": int(frame[LABEL_COL].sum()),\n",
    "            \"roc_auc\": float(roc_auc_score(yt, pt)) if len(np.unique(yt)) > 1 else np.nan,\n",
    "            \"pr_auc\": float(average_precision_score(yt, pt)) if len(np.unique(yt)) > 1 else np.nan,\n",
    "        }\n",
    "        return pd.Series(out)\n",
    "\n",
    "    by_gender = df_test_sub.groupby(\"gender\", dropna=False).apply(group_metrics).reset_index()\n",
    "    by_age = df_test_sub.groupby(\"age_bin\", dropna=False).apply(group_metrics).reset_index()\n",
    "\n",
    "    print(\"Metrics by gender:\")\n",
    "    display(by_gender)\n",
    "\n",
    "    print(\"Metrics by age_bin:\")\n",
    "    display(by_age)\n",
    "else:\n",
    "    print(\"patients.csv not found; skipping subgroup checks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb467b",
   "metadata": {},
   "source": [
    "## 12) Save evaluation artifacts\n",
    "\n",
    "We save:\n",
    "- `eval_predictions.csv`: identifiers + true label + predicted probability on the test split\n",
    "- `threshold_report.csv`: precision/recall/F1 across thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f17f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_OUT = Path(\"eval_predictions.csv\")\n",
    "THR_OUT = Path(\"threshold_report.csv\")\n",
    "\n",
    "df_test_out = df_test[[\"subject_id\", \"hadm_id\", \"stay_id\", LABEL_COL, \"pred_proba\"]].copy()\n",
    "df_test_out.to_csv(PRED_OUT, index=False)\n",
    "report.to_csv(THR_OUT, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" \", PRED_OUT.resolve())\n",
    "print(\" \", THR_OUT.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
